<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo</title>
  <link rel="icon" type="image/x-icon" href="static/images/soldier.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://zongrui.page" target="_blank">Zongrui Li</a><sup>1, *</sup>,</span>
              <span class="author-block">
              Zhan Lu<sup>1, 2, *</sup>,</span>
              <span class="author-block">
              <span class="author-block">
              Haojie Yan<sup>2</sup>,</span>
              <span class="author-block">
              <span class="author-block">
                <a href="https://ci.idm.pku.edu.cn/" target="_blank">Boxin Shi</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://person.zju.edu.cn/en/gpan" target="_blank">Gang Pan</a><sup>2</sup>,
              </span>
                  <a href="https://person.zju.edu.cn/zq" target="_blank">Qian Zheng</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://personal.ntu.edu.sg/exdjiang/" target="_blank">Xudong Jiang</a><sup>1</sup>,
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><font size="4"><sup>1</sup>Nanyang Technological University, <sup>2</sup>Zhejiang University, <sup>3</sup>Peking University</font> <br>CVPR 2024</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/abs/2404.01612" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/LMozart/CVPR2024-SpinUP" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2404.01612" 
                  target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Natural Light Uncalibrated Photometric Stereo (NaUPS) relieves the strict environment and light assumptions in classical Uncalibrated Photometric Stereo (UPS) methods. However, due to the intrinsic ill-posedness and high-dimensional ambiguities, addressing NaUPS is still an open question. Existing works impose strong assumptions on the environment lights and objects' material, restricting the effectiveness in more general scenarios. Alternatively, some methods leverage supervised learning with intricate models while lacking interpretability, resulting in a biased estimation. In this work, we proposed Spin Light Uncalibrated Photometric Stereo (Spin-UP), an unsupervised method to tackle NaUPS in various environment lights and objects. The proposed method uses a novel setup that captures the object's images on a rotatable platform, which mitigates NaUPS's ill-posedness by reducing unknowns and provides reliable priors to alleviate NaUPS's ambiguities. Leveraging neural inverse rendering and the proposed training strategies, Spin-UP recovers surface normals, environment light, and isotropic reflectance under complex natural light with low computational cost. Experiments have shown that Spin-UP outperforms other supervised / unsupervised NaUPS methods and achieves state-of-the-art performance on synthetic and real-world datasets.
          </p>
        </div>
      </div>
    </div>
  </div>

</section>
<!-- End paper abstract -->

<!-- <section class="section hero is-light"> -->
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            The proposed Spin-UP handles natural light photometric stereo based on a new setup. We highlight our contribution as: 
            <ol>
              <li> We design a <b>novel setup</b> for NaUPS, which reduces unknowns of light representation and facilitates solving NaUPS in an unsupervised manner; </li> 
              <li> We introduce a <b>light prior</b>, which leverages an object’s occluding boundaries to initialize a reliable environment light. Based on the setup and light prior, we propose the unsupervised NaUPS method named Spin-UP; </li> 
              <li> We present <b>two training strategies</b> for fast training and convergence of Spin-UP; </li> 
            </ol>
            The proposed Spin-UP produces highly accurate surface normal given images of object captured under natural light in an unsupervised manner, even comparable to the SOTA supervised method.
          </p>
        </div>
           <img src="./static/images/teaser.png" alt="Teaser" style="width:90%;">
        </div>
        </div>
    </div>
  </div>
<!-- </section> -->
<br>
<br>
<br>

<!-- <section class="section hero is-light"> -->
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Spin Light Setup</h2>
        <div class="content has-text-justified">
          <p>
            We record a video for an object under an environment light by rotating the object together with a linear, perspective camera in 360° on a rotatable platform. The relative positions and orientations between camera and object remain fixed during rotation. 
          </p>
        </div>
          <!-- <img src="./static/images/spin_light.gif" alt="Teaser" style="width:70%;"> -->
          <video src="./static/images/spin_light.mp4" controls="controls" style="width:70%;"></video>
          <div class="content has-text-justified">
            <p>
              We then extract around 50 images from the recorded video, regarding as our training data. 
            </p>
          </div>
          <img src="./static/images/image_batch.png" alt="Teaser" style="width:100%;">
      </div>
    </div>
    
  </div>
<!-- </section> -->

<br>
<br>
<br>


<!-- <section class="section hero is-light"> -->
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Light Initialization Method</h2>
        <div class="content has-text-justified">
          <p>
            Based on the proposed setup, we derive a rough light map given the observed images.
          </p>
        </div>
          <!-- <img src="./static/images/light_prior.gif" alt="Teaser" style="width:90%;"> -->
          <video src="./static/images/light_prior.mp4" controls="controls" style="width:90%;"></video>
      </div>
    </div>
  </div>
<!-- </section> -->

<br>
<br>
<br>

<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Training Strategies</h2>
      <div class="content has-text-justified">
        <p>
          We proposed two training strategies (top: <b>interval sampling</b>, bottom: <b>shrinking range computing</b>) to improve the training efficiency and alleviate aliasing issues.
        </p>
      </div>
        <img src="./static/images/training_strategies.png" alt="Teaser" style="width:70%;">
    </div>
  </div>
</div>
<br>
<br>
<br>
<!-- https://www.youtube.com/watch?v=z1ujvl9ePkQ -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video Presentation</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/z1ujvl9ePkQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
<br>
<br>
<br>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{li2024spinup,
      title={Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo},
      author={Li, Zongrui and Lu, Zhan and Yan, Haojie and Shi, Boxin and Pan, Gang and Zheng, Qian and Jiang, Xudong},
      booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
      year={2024}}
</code></pre>
  </div>
</section>

  </body>
  </html>
